## 📚 **🌟 Theoretical Explanation: How K-Nearest Neighbors (KNN) Regression Works**

---

### ✅ **1. What is KNN Regression?**

**KNN Regression** is a **non-parametric supervised learning method** used to predict **continuous target variables**.

✅ **Key idea:**
It predicts the output for a new input **based on the average of the target values of its K nearest neighbors** in the training data.

---

### 🔹 **2. How Does KNN Regression Work?**

#### **Step-by-Step Process:**

✅ **Step 1: Choose K**

* Select **K**, the number of nearest neighbors to consider (e.g. K=3, 5, or 7).

---

✅ **Step 2: Calculate Distance**

* For a new input data point:

  * Calculate the **distance between this point and all points in the training data**.

  **Common distance metrics:**

  | **Metric** | **Formula**                  | **Use case**                     |    |                                   |
  | ---------- | ---------------------------- | -------------------------------- | -- | --------------------------------- |
  | Euclidean  | $\sqrt{\sum{(x_i - x_j)^2}}$ | Default for continuous variables |    |                                   |
  | Manhattan  | (\sum{                       | x\_i - x\_j                      | }) | Grid-like data (e.g. city blocks) |

---

✅ **Step 3: Identify K Nearest Neighbors**

* Sort all distances in **ascending order**.
* Select the **K points with smallest distances** to the new input.

---

✅ **Step 4: Predict Target Value**

* Calculate the **average (mean) of the target values of these K neighbors**.

  $$
  y_{predicted} = \frac{1}{K} \sum_{i=1}^{K} y_i
  $$

✅ **Optional:**
Use **weighted averaging** where closer neighbors have more influence (weights inversely proportional to distance).

---

### 🔹 **3. Intuition Behind KNN Regression**

✅ **No assumption about data distribution**:

* Unlike linear regression or SVR, KNN does not assume linearity or any functional form between inputs and outputs.

✅ **Prediction is local:**

* Based purely on **nearby data points**, making it powerful for datasets with complex local patterns.

---

### 🔹 **4. Hyperparameters**

| **Parameter**        | **Role**                                                                                                       |
| -------------------- | -------------------------------------------------------------------------------------------------------------- |
| **K (n\_neighbors)** | Number of neighbors to consider. Smaller K → more variance (overfitting), larger K → more bias (underfitting). |
| **weights**          | ‘uniform’ (equal weight) or ‘distance’ (closer neighbors have higher weight).                                  |
| **metric**           | Distance metric used.                                                                                          |

---

### 🔹 **5. Advantages**

✅ **Strengths:**

* **Simple to understand and implement.**
* No need for model training (lazy learner).
* Works well for **non-linear relationships** and data with local clusters.

---

### 🔹 **6. Limitations**

⚠️ **Weaknesses:**

* **Computationally expensive** at prediction time (calculates distance to all training points).
* Suffers from **curse of dimensionality** (performance degrades with many features).
* Sensitive to **feature scaling** (standardization is essential).
* No model interpretability – purely memory-based approach.

---

### 🔹 **7. Applicability**

| **When to use?**                          | **When not to use?**                     |
| ----------------------------------------- | ---------------------------------------- |
| Small to medium datasets                  | Very large datasets (slow predictions)   |
| Non-linear regression with local patterns | High-dimensional data (poor performance) |
| No assumptions about data distribution    | When model interpretability is required  |

---

### 🔹 **8. Example Intuition**

**If K=3**:

| **Neighbors** | **Distance** | **Target Price** |
| ------------- | ------------ | ---------------- |
| House A       | 100          | \$250,000        |
| House B       | 150          | \$270,000        |
| House C       | 200          | \$260,000        |

✅ **Prediction:**

$$
Predicted Price = \frac{250,000 + 270,000 + 260,000}{3} = 260,000
$$

---

### 🔹 **9. Summary of Working Principle**

| **Step**                   | **Explanation**                                    |
| -------------------------- | -------------------------------------------------- |
| **1. Choose K**            | Number of neighbors.                               |
| **2. Calculate distances** | Between new input and all training data.           |
| **3. Select K nearest**    | Data points with smallest distances.               |
| **4. Predict output**      | Average their target values (weighted or uniform). |

---

### 🔹 **10. KNN Regression vs KNN Classification**

| **Regression**                                | **Classification**                                               |
| --------------------------------------------- | ---------------------------------------------------------------- |
| Predicts **mean of neighbors' target values** | Predicts **most frequent class (majority vote)** among neighbors |
| Output is continuous                          | Output is categorical                                            |

---


## ✅ **🔷 Step By Step Working with Python**

---
### 📝 **🔹 Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area and number of bedrooms** using **Decision Tree Regression**.

---

### 🔹 **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

---

### 🔹 **Step 2: Create Sample Data**

```python
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

---

### 🔹 **Step 3: Prepare Data**

```python
X = df[['Area', 'Bedrooms']]
y = df['Price']
```


### 📝 **🔹 Step 4: Import Library**

```python
from sklearn.neighbors import KNeighborsRegressor
```

---

### 🔹 **Step 5: Fit Model**

```python
model = KNeighborsRegressor(n_neighbors=3)
model.fit(X, y)
```

---

### 🔹 **Step 6: Make Predictions**

```python
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### 🔹 **Step 7: Applicability**

| **When to use?**                | **When not to use?**                            |
| ------------------------------- | ----------------------------------------------- |
| Simple, local prediction needed | High dimensional data (curse of dimensionality) |

---
## 📚 **🌟 Full Stepwise Case Study: K-Nearest Neighbors (KNN) Regression**

---

## ✅ **🔷 Case Study Topic**

**🔹 Predicting House Prices Using KNN Regression**

---

### 📝 **🔹 Problem Statement**

You are given a dataset of **houses** with the following features:

* **Area:** Area in square feet
* **Bedrooms:** Number of bedrooms
* **Bathrooms:** Number of bathrooms
* **Stories:** Number of floors
* **Parking:** Number of parking spaces
* **Price:** House price (Target)

**Objective:**
Build a **KNN Regression model** to predict **house prices** based on these features.

---

### 🔹 **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
```

---

### 🔹 **Step 2: Create Simulated Dataset**

We will use the same dataset as before for consistency:

```python
# Simulate data
np.random.seed(42)
data = {
    'Area': np.random.randint(500, 3500, 100),
    'Bedrooms': np.random.randint(1, 5, 100),
    'Bathrooms': np.random.randint(1, 4, 100),
    'Stories': np.random.randint(1, 3, 100),
    'Parking': np.random.randint(0, 3, 100)
}

df = pd.DataFrame(data)
# Create target variable with some random noise
df['Price'] = (df['Area'] * 300) + (df['Bedrooms'] * 50000) + (df['Bathrooms'] * 30000) + (df['Stories'] * 25000) + (df['Parking'] * 15000) + np.random.randint(-20000, 20000, 100)

print(df.head())
```

---

### 🔹 **Step 3: Define Features and Target**

```python
X = df.drop('Price', axis=1)
y = df['Price']
```

---

### 🔹 **Step 4: Train-Test Split**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

### 🔹 **Step 5: Feature Scaling**

✅ **Why?**
KNN is a **distance-based algorithm**, requiring standardization for correct results:

```python
sc_X = StandardScaler()
X_train_scaled = sc_X.fit_transform(X_train)
X_test_scaled = sc_X.transform(X_test)
```

---

### 🔹 **Step 6: Fit KNN Regressor**

```python
model = KNeighborsRegressor(n_neighbors=5)
model.fit(X_train_scaled, y_train)
```

✅ **Hyperparameter Explanation:**

| **Parameter** | **Role**                                                        |
| ------------- | --------------------------------------------------------------- |
| `n_neighbors` | Number of neighbors to consider for prediction                  |
| `weights`     | ‘uniform’ or ‘distance’ based weighting of neighbors            |
| `metric`      | Distance metric (default is ‘minkowski’ with p=2 for Euclidean) |

---

### 🔹 **Step 7: Make Predictions**

```python
y_pred = model.predict(X_test_scaled)
print(y_pred[:5])
```

---

### 🔹 **Step 8: Evaluate Model**

```python
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

✅ **Interpretation:**

* **MSE:** Average squared difference between predicted and actual prices.
* **R²:** Percentage of variance explained by the model.

---

### 🔹 **Step 9: Choosing Optimal K**

Test different K values to find the best:

```python
errors = []
for k in range(1, 21):
    model = KNeighborsRegressor(n_neighbors=k)
    model.fit(X_train_scaled, y_train)
    y_pred_k = model.predict(X_test_scaled)
    mse_k = mean_squared_error(y_test, y_pred_k)
    errors.append(mse_k)

plt.plot(range(1, 21), errors, marker='o')
plt.xlabel('Number of Neighbors K')
plt.ylabel('Mean Squared Error')
plt.title('Choosing Optimal K in KNN Regression')
plt.show()
```

✅ **Interpretation:**
Choose **K with minimum MSE** for best performance.

---

### 🔹 **Step 10: Applicability of KNN Regression**

| **When to use?**                        | **When not to use?**                                |
| --------------------------------------- | --------------------------------------------------- |
| Simple, non-parametric regression tasks | High-dimensional datasets (curse of dimensionality) |
| Data has **local patterns**             | Large datasets (computational cost of predictions)  |
| No assumption of linearity              |                                                     |

---

### 🔹 **Step 11: Advantages and Limitations**

✅ **Advantages:**

* Simple to implement and understand
* No assumption about data distribution

⚠️ **Limitations:**

* Computationally intensive for large datasets
* Sensitive to choice of K and feature scaling
* Poor performance in **high dimensions**

---

## ✅ **End of Full Case Study**

---
