# üìö **üåü Elastic Net Regression (Combination of L1 & L2 Regularization)**

### **Introduction to Elastic Net Regression (Combination of L1 & L2 Regularization)**

Elastic Net Regression is an advanced regularization technique that combines the strengths of **Lasso Regression (L1 Regularization)** and **Ridge Regression (L2 Regularization)**. It is particularly useful when dealing with **high-dimensional datasets** where predictors are numerous and often highly correlated.

**Key aspects of Elastic Net Regression include:**

* **Equation Form:**
  The objective function for Elastic Net is:

  $$
  \text{Minimize} \; \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + Œª_1 \sum_{j=1}^{p}|Œ≤_j| + Œª_2 \sum_{j=1}^{p}Œ≤_j^2
  $$

  or in a combined form with mixing parameter *Œ±* (between 0 and 1):

  $$
  \text{Minimize} \; \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + Œª \left[ Œ± \sum_{j=1}^{p}|Œ≤_j| + (1-Œ±) \sum_{j=1}^{p}Œ≤_j^2 \right]
  $$

  where:

  * *y·µ¢* are the observed values,
  * *≈∑·µ¢* are the predicted values,
  * *Œ≤\_j* are the model coefficients,
  * *Œª* is the regularization strength,
  * *Œ±* controls the mix between L1 (Lasso) and L2 (Ridge) penalties.

* **Interpretation:**

  * Elastic Net combines **L1‚Äôs feature selection** capability and **L2‚Äôs coefficient shrinkage** to handle datasets where:

    * Predictors are highly correlated, or
    * The number of predictors exceeds the number of observations.
  * It avoids the limitation of Lasso where, in the presence of highly correlated features, only one is selected arbitrarily.

* **Key advantages:**

  * **Feature selection with stability:** Unlike Lasso, Elastic Net can retain groups of correlated predictors rather than selecting just one.
  * **Flexibility:** By tuning *Œ±*, you can adjust the model to behave more like Ridge (*Œ±=0*) or Lasso (*Œ±=1*) based on the problem needs.
  * **Better generalization:** Helps reduce overfitting by balancing bias and variance.

* **Assumptions:**
  Similar to linear regression:

  * Linearity between predictors and response
  * Independence of observations
  * Homoscedasticity (constant variance of residuals)
  * Normally distributed residuals

* **Applications:**
  Elastic Net is widely used when:

  * There are **many predictors with multicollinearity**.
  * The dataset is **high-dimensional**, such as in genomics, text mining, or economic modeling.
  * A balance between variable selection (L1) and coefficient stability (L2) is required for optimal model performance.

Overall, Elastic Net Regression provides a **robust and versatile regularization approach**, combining the benefits of both Lasso and Ridge, making it a popular choice in machine learning and statistical modeling for complex data scenarios.

### üìù **üîπ Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **multiple features**, combining the benefits of **Ridge (L2)** and **Lasso (L1)** regularizations.

---

### üîπ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
```

---

### üîπ **Step 2: Create Sample Data**

```python
# Sample dataset with multiple features
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Age': [5, 7, 10, 12, 15],
    'Distance': [2, 3, 5, 7, 8],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

‚úÖ **Why Elastic Net Regression?**

* When:

  * There are **multiple correlated predictors**.
  * You want **feature selection (Lasso)** plus **stability (Ridge)**.

---

### üîπ **Step 3: Prepare Data**

```python
X = df[['Area', 'Bedrooms', 'Age', 'Distance']]
y = df['Price']
```

---

### üîπ **Step 4: Fit Elastic Net Regression Model**

```python
model = ElasticNet(alpha=1.0, l1_ratio=0.5)  # l1_ratio=0.5 means equal balance of L1 and L2
model.fit(X, y)
```

‚úÖ **Notes:**

* **alpha:** Overall regularization strength.
* **l1\_ratio:**

  * **0:** Equivalent to Ridge.
  * **1:** Equivalent to Lasso.
  * **0.5:** Equal combination.

---

### üîπ **Step 5: Check Model Parameters**

```python
print(f"Intercept: {model.intercept_}")
print(f"Coefficients: {model.coef_}")
```

‚úÖ **Interpretation:**

* Some coefficients may be **shrunk to zero** (feature selection) while others are **regularized** (controlled).

---

### üîπ **Step 6: Make Predictions**

```python
new_data = pd.DataFrame({
    'Area': [1100, 1600],
    'Bedrooms': [2, 3],
    'Age': [9, 14],
    'Distance': [4, 7]
})
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### üîπ **Step 7: Evaluate Model**

```python
y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

---

### üîπ **Step 8: Applicability of Elastic Net Regression**

| **When to use?**                                                    | **When not to use?**                    |
| ------------------------------------------------------------------- | --------------------------------------- |
| **Many correlated variables** with desire for **feature selection** | Pure Ridge or Lasso is sufficient       |
| Model needs **stability + sparsity**                                | Small dataset with no multicollinearity |
| High-dimensional data (more predictors than samples)                |                                         |

---

### üìù **End of Part 6**

---
