# üìö **üåü Lasso Regression (L1 Regularization)**

### **Introduction to Lasso Regression (L1 Regularization)**

Lasso Regression, also known as **L1 Regularization**, is an advanced linear regression technique that adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. It is particularly useful for **feature selection** and preventing overfitting in models with many predictors.

**Key aspects of Lasso Regression include:**

* **Equation Form:**
  The objective function for Lasso Regression is:

  $$
  \text{Minimize} \; \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + Œª \sum_{j=1}^{p}|Œ≤_j|
  $$

  where:

  * *y·µ¢* are the observed values,
  * *≈∑·µ¢* are the predicted values,
  * *Œ≤\_j* are the model coefficients,
  * *Œª* (lambda) is the **regularization parameter**, controlling the strength of the penalty.

* **Interpretation:**

  * The penalty term (*Œª Œ£ |Œ≤\_j|*) forces some coefficients to become exactly zero when *Œª* is sufficiently large.
  * This results in **sparse models** where only the most significant predictors remain, effectively performing variable selection.
  * When *Œª* is zero, Lasso reduces to ordinary linear regression.

* **Key advantages:**

  * **Feature selection:** Automatically eliminates irrelevant or less significant variables by shrinking their coefficients to zero.
  * **Reduces overfitting:** Helps in creating simpler models with better generalization on unseen data.

* **Limitations:**

  * When there are highly correlated predictors, Lasso tends to select one of them and ignore the others, which may not always be desirable.
  * The selection among correlated features is arbitrary.

* **Assumptions:**
  Same as linear regression:

  * Linearity of relationships
  * Independence of observations
  * Homoscedasticity (constant variance of residuals)
  * Normally distributed residuals

* **Applications:**
  Lasso Regression is widely used for:

  * **High-dimensional data analysis**, where the number of predictors is large relative to observations (e.g. genomic data, text data).
  * **Feature selection** before implementing more complex models.
  * Improving interpretability by reducing the number of variables in the final model.

Due to its capability to perform **both regularization and variable selection**, Lasso Regression is an essential tool in statistical learning and machine learning for building efficient and interpretable models.


### üìù **üîπ Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **multiple features**, while **selecting only the important predictors** (some coefficients may become zero).

---

### üîπ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score
```

---

### üîπ **Step 2: Create Sample Data**

```python
# Sample dataset with multiple features
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Age': [5, 7, 10, 12, 15],
    'Distance': [2, 3, 5, 7, 8],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

‚úÖ **Why Lasso Regression?**

* **Performs feature selection** by shrinking some coefficients to zero, keeping only significant predictors.

---

### üîπ **Step 3: Prepare Data**

```python
X = df[['Area', 'Bedrooms', 'Age', 'Distance']]
y = df['Price']
```

---

### üîπ **Step 4: Fit Lasso Regression Model**

```python
model = Lasso(alpha=1.0)  # alpha controls regularization strength
model.fit(X, y)
```

‚úÖ **Note:**

* **Higher alpha:** More coefficients set to zero (stronger selection).
* **Lower alpha:** Closer to linear regression.

---

### üîπ **Step 5: Check Model Parameters**

```python
print(f"Intercept: {model.intercept_}")
print(f"Coefficients: {model.coef_}")
```

‚úÖ **Interpretation:**

* Coefficients with **zero value** are **excluded from the model**.

---

### üîπ **Step 6: Make Predictions**

```python
new_data = pd.DataFrame({
    'Area': [1100, 1600],
    'Bedrooms': [2, 3],
    'Age': [9, 14],
    'Distance': [4, 7]
})
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### üîπ **Step 7: Evaluate Model**

```python
y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

---

### üîπ **Step 8: Applicability of Lasso Regression**

| **When to use?**                             | **When not to use?**                            |
| -------------------------------------------- | ----------------------------------------------- |
| **Feature selection** is needed              | When all predictors are needed                  |
| Large number of predictors, some unimportant | Predictors are highly correlated (Ridge better) |
| Sparse model desired                         |                                                 |

---

### üìù **End of Part 5**

---
