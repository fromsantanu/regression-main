# 📚 **🌟 Ridge Regression (L2 Regularization)**

### **Introduction to Ridge Regression (L2 Regularization)**

Ridge Regression, also known as **L2 Regularization**, is a type of linear regression technique that adds a penalty term to the ordinary least squares (OLS) loss function to prevent **overfitting**, especially when dealing with multicollinearity or a large number of predictors.

**Key aspects of Ridge Regression include:**

* **Equation Form:**
  The objective function for Ridge Regression is:

  $\  \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + λ \sum_{j=1}^{p}β_j^2 \$

  where:

  * *yᵢ* are the observed values,
  * *ŷᵢ* are the predicted values,
  * *β\_j* are the model coefficients,
  * *λ* (lambda) is the **regularization parameter**, controlling the strength of the penalty.

* **Interpretation:**

  * The penalty term (*λ Σ β\_j²*) **shrinks the coefficients** towards zero but does not force them to become exactly zero.
  * As *λ* increases, the model becomes simpler (coefficients shrink more), reducing variance but increasing bias slightly.
  * When *λ* is zero, Ridge Regression reduces to ordinary linear regression.

* **Key advantages:**

  * **Reduces overfitting:** By penalizing large coefficients, it prevents the model from fitting noise in the data.
  * **Handles multicollinearity:** Ridge Regression is particularly effective when independent variables are highly correlated, stabilizing coefficient estimates.

* **Assumptions:**
  Similar to linear regression:

  * Linearity between predictors and response
  * Independence of observations
  * Homoscedasticity (constant variance of residuals)
  * Normally distributed residuals

* **Applications:**
  Ridge Regression is widely used when:

  * The dataset has **many correlated predictors** (e.g. gene expression data, economic indicators).
  * A model with **better generalization** is needed, trading off some interpretability for predictive accuracy.

Due to its regularization capability, Ridge Regression forms a foundational method in machine learning for improving model performance, especially in high-dimensional datasets where overfitting is a concern.

### 📝 **🔹 Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **multiple features**, with **potential multicollinearity**.

---

### 🔹 **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
```

---

### 🔹 **Step 2: Create Sample Data**

```python
# Sample dataset with multicollinearity
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Age': [5, 7, 10, 12, 15],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

✅ **Why Ridge Regression?**

* When predictors are **highly correlated**, **Ridge reduces overfitting** by adding a penalty on coefficients.

---

### 🔹 **Step 3: Prepare Data**

```python
X = df[['Area', 'Bedrooms', 'Age']]
y = df['Price']
```

---

### 🔹 **Step 4: Fit Ridge Regression Model**

```python
model = Ridge(alpha=1.0)  # alpha is the regularization strength
model.fit(X, y)
```

✅ **Note:**

* **Higher alpha:** More penalty, smaller coefficients
* **Lower alpha (→0):** Similar to linear regression

---

### 🔹 **Step 5: Check Model Parameters**

```python
print(f"Intercept: {model.intercept_}")
print(f"Coefficients: {model.coef_}")
```

---

### 🔹 **Step 6: Make Predictions**

```python
new_data = pd.DataFrame({
    'Area': [1100, 1600],
    'Bedrooms': [2, 3],
    'Age': [9, 14]
})
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### 🔹 **Step 7: Evaluate Model**

```python
y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

---

### 🔹 **Step 8: Applicability of Ridge Regression**

| **When to use?**                                 | **When not to use?**                            |
| ------------------------------------------------ | ----------------------------------------------- |
| Predicting with **multicollinear predictors**    | When **feature selection** is needed            |
| Avoiding **overfitting in complex models**       | If coefficients need to become **exactly zero** |
| Many correlated variables with small sample size |                                                 |

---

### 📝 **End of Part 4**

---
