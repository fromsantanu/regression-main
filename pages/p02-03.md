# ğŸ“š **ğŸŒŸ Ridge Regression (L2 Regularization)**

---

### ğŸ“ **ğŸ”¹ Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **multiple features**, with **potential multicollinearity**.

---

### ğŸ”¹ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
```

---

### ğŸ”¹ **Step 2: Create Sample Data**

```python
# Sample dataset with multicollinearity
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Age': [5, 7, 10, 12, 15],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

âœ… **Why Ridge Regression?**

* When predictors are **highly correlated**, **Ridge reduces overfitting** by adding a penalty on coefficients.

---

### ğŸ”¹ **Step 3: Prepare Data**

```python
X = df[['Area', 'Bedrooms', 'Age']]
y = df['Price']
```

---

### ğŸ”¹ **Step 4: Fit Ridge Regression Model**

```python
model = Ridge(alpha=1.0)  # alpha is the regularization strength
model.fit(X, y)
```

âœ… **Note:**

* **Higher alpha:** More penalty, smaller coefficients
* **Lower alpha (â†’0):** Similar to linear regression

---

### ğŸ”¹ **Step 5: Check Model Parameters**

```python
print(f"Intercept: {model.intercept_}")
print(f"Coefficients: {model.coef_}")
```

---

### ğŸ”¹ **Step 6: Make Predictions**

```python
new_data = pd.DataFrame({
    'Area': [1100, 1600],
    'Bedrooms': [2, 3],
    'Age': [9, 14]
})
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### ğŸ”¹ **Step 7: Evaluate Model**

```python
y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

---

### ğŸ”¹ **Step 8: Applicability of Ridge Regression**

| **When to use?**                                 | **When not to use?**                            |
| ------------------------------------------------ | ----------------------------------------------- |
| Predicting with **multicollinear predictors**    | When **feature selection** is needed            |
| Avoiding **overfitting in complex models**       | If coefficients need to become **exactly zero** |
| Many correlated variables with small sample size |                                                 |

---

### ğŸ“ **End of Part 4**

---
