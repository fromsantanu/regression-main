## âœ… **ğŸ”· Section 6: Neural Network Regression**

---

### ğŸ“ **ğŸ”¹ Step 0: Import Libraries**

```python
from sklearn.neural_network import MLPRegressor
```

---

### ğŸ”¹ **Step 1: Fit Model**

```python
model = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=1000, random_state=0)
model.fit(X, y)
```

---

### ğŸ”¹ **Step 2: Make Predictions**

```python
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### ğŸ”¹ **Step 3: Applicability**

| **When to use?**             | **When not to use?**    |
| ---------------------------- | ----------------------- |
| Complex, non-linear patterns | Small datasets          |
| Deep learning tasks          | Interpretability needed |

---

### ğŸ“š **ğŸŒŸ Full Stepwise Case Study: Neural Network Regression**

---

## âœ… **ğŸ”· Case Study Topic**

**ğŸ”¹ Predicting House Prices Using Neural Network Regression (MLP Regressor)**

---

### ğŸ“ **ğŸ”¹ Problem Statement**

You are given a dataset of **houses** with the following features:

* **Area:** Area in square feet
* **Bedrooms:** Number of bedrooms
* **Bathrooms:** Number of bathrooms
* **Stories:** Number of floors
* **Parking:** Number of parking spaces
* **Price:** House price (Target)

**Objective:**
Build a **Neural Network Regression model** to predict **house prices** based on these features.

---

### ğŸ”¹ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
```

---

### ğŸ”¹ **Step 2: Create Simulated Dataset**

Using the same dataset for consistency:

```python
# Simulate data
np.random.seed(42)
data = {
    'Area': np.random.randint(500, 3500, 100),
    'Bedrooms': np.random.randint(1, 5, 100),
    'Bathrooms': np.random.randint(1, 4, 100),
    'Stories': np.random.randint(1, 3, 100),
    'Parking': np.random.randint(0, 3, 100)
}

df = pd.DataFrame(data)
# Create target variable with some random noise
df['Price'] = (df['Area'] * 300) + (df['Bedrooms'] * 50000) + (df['Bathrooms'] * 30000) + (df['Stories'] * 25000) + (df['Parking'] * 15000) + np.random.randint(-20000, 20000, 100)

print(df.head())
```

---

### ğŸ”¹ **Step 3: Define Features and Target**

```python
X = df.drop('Price', axis=1)
y = df['Price']
```

---

### ğŸ”¹ **Step 4: Train-Test Split**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

### ğŸ”¹ **Step 5: Feature Scaling**

âœ… **Why?**
Neural Networks require **scaled input features** for efficient training:

```python
sc_X = StandardScaler()
X_train_scaled = sc_X.fit_transform(X_train)
X_test_scaled = sc_X.transform(X_test)

# Scale target as well if needed, but here we keep y in original scale for interpretation
```

---

### ğŸ”¹ **Step 6: Fit Neural Network (MLP Regressor) Model**

```python
model = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=1000, random_state=42)
model.fit(X_train_scaled, y_train)
```

âœ… **Hyperparameter Explanation:**

| **Parameter**        | **Role**                                                                                 |
| -------------------- | ---------------------------------------------------------------------------------------- |
| `hidden_layer_sizes` | Tuple indicating neurons per hidden layer (e.g. (10,10) = two layers of 10 neurons each) |
| `activation`         | Activation function (default â€˜reluâ€™)                                                     |
| `solver`             | Optimization algorithm (â€˜adamâ€™ default)                                                  |
| `learning_rate`      | Learning rate strategy (â€˜constantâ€™, â€˜adaptiveâ€™)                                          |
| `max_iter`           | Maximum iterations for convergence                                                       |

---

### ğŸ”¹ **Step 7: Make Predictions**

```python
y_pred = model.predict(X_test_scaled)
print(y_pred[:5])
```

---

### ğŸ”¹ **Step 8: Evaluate Model**

```python
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

âœ… **Interpretation:**

* **MSE:** Average squared difference between predicted and actual prices.
* **RÂ²:** Percentage of variance explained by the model.

---

### ğŸ”¹ **Step 9: Learning Curve (Optional)**

To check convergence:

```python
plt.plot(model.loss_curve_)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Neural Network Training Loss Curve')
plt.show()
```

---

### ğŸ”¹ **Step 10: Applicability of Neural Network Regression**

| **When to use?**                  | **When not to use?**                 |
| --------------------------------- | ------------------------------------ |
| Complex non-linear patterns exist | Small datasets (risk of overfitting) |
| Large datasets with many features | Interpretability is important        |
| Need for powerful approximators   |                                      |

---

### ğŸ”¹ **Step 11: Advantages and Limitations**

âœ… **Advantages:**

* Captures **complex, non-linear relationships**
* Flexible model architecture

âš ï¸ **Limitations:**

* Requires **large datasets** for stable training
* Acts as a **black box** with low interpretability
* Sensitive to **feature scaling** and parameter tuning

---

## âœ… **End of Full Case Study**

---

## ğŸ“ **ğŸ”· Final Summary**

| **Method**        | **Key Advantage**                      | **Limitation**                       |
| ----------------- | -------------------------------------- | ------------------------------------ |
| Decision Tree     | Easy interpretation                    | Overfitting                          |
| Random Forest     | High accuracy, robust                  | Less interpretable                   |
| SVR               | High dimensional non-linear regression | Computational cost                   |
| KNN               | Simple, local                          | Poor in high dimensions              |
| Gradient Boosting | High predictive power                  | Slower training                      |
| Neural Networks   | Captures complex patterns              | Needs large data, less interpretable |

---


