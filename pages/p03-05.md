## ✅ **🔷 Section 5: Gradient Boosting Regression**

---

### 📝 **🔹 Step 0: Import Library**

```python
from sklearn.ensemble import GradientBoostingRegressor
```

---

### 🔹 **Step 1: Fit Model**

```python
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=0)
model.fit(X, y)
```

---

### 🔹 **Step 2: Make Predictions**

```python
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### 🔹 **Step 3: Applicability**

| **When to use?**                 | **When not to use?**     |
| -------------------------------- | ------------------------ |
| Highest accuracy needed          | Extremely small datasets |
| Non-linear complex relationships |                          |

---
### 📚 **🌟 Full Stepwise Case Study: Gradient Boosting Regression**

---

## ✅ **🔷 Case Study Topic**

**🔹 Predicting House Prices Using Gradient Boosting Regression**

---

### 📝 **🔹 Problem Statement**

You are given a dataset of **houses** with the following features:

* **Area:** Area in square feet
* **Bedrooms:** Number of bedrooms
* **Bathrooms:** Number of bathrooms
* **Stories:** Number of floors
* **Parking:** Number of parking spaces
* **Price:** House price (Target)

**Objective:**
Build a **Gradient Boosting Regression model** to predict **house prices** based on these features.

---

### 🔹 **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
```

---

### 🔹 **Step 2: Create Simulated Dataset**

Using the same dataset for consistency:

```python
# Simulate data
np.random.seed(42)
data = {
    'Area': np.random.randint(500, 3500, 100),
    'Bedrooms': np.random.randint(1, 5, 100),
    'Bathrooms': np.random.randint(1, 4, 100),
    'Stories': np.random.randint(1, 3, 100),
    'Parking': np.random.randint(0, 3, 100)
}

df = pd.DataFrame(data)
# Create target variable with some random noise
df['Price'] = (df['Area'] * 300) + (df['Bedrooms'] * 50000) + (df['Bathrooms'] * 30000) + (df['Stories'] * 25000) + (df['Parking'] * 15000) + np.random.randint(-20000, 20000, 100)

print(df.head())
```

---

### 🔹 **Step 3: Define Features and Target**

```python
X = df.drop('Price', axis=1)
y = df['Price']
```

---

### 🔹 **Step 4: Train-Test Split**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

### 🔹 **Step 5: Feature Scaling (Optional)**

✅ Gradient Boosting trees **do not require feature scaling**, but it’s good practice for comparing with methods that do:

```python
# Not mandatory for tree-based models
# sc_X = StandardScaler()
# X_train = sc_X.fit_transform(X_train)
# X_test = sc_X.transform(X_test)
```

---

### 🔹 **Step 6: Fit Gradient Boosting Regressor**

```python
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
model.fit(X_train, y_train)
```

✅ **Hyperparameter Explanation:**

| **Parameter**   | **Role**                          |
| --------------- | --------------------------------- |
| `n_estimators`  | Number of boosting stages (trees) |
| `learning_rate` | Shrinks contribution of each tree |
| `max_depth`     | Maximum depth of each tree        |

---

### 🔹 **Step 7: Make Predictions**

```python
y_pred = model.predict(X_test)
print(y_pred[:5])
```

---

### 🔹 **Step 8: Evaluate Model**

```python
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

✅ **Interpretation:**

* **MSE:** Average squared difference between predicted and actual prices.
* **R²:** Percentage of variance explained by the model.

---

### 🔹 **Step 9: Feature Importance**

```python
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=False).plot(kind='bar')
plt.title('Feature Importance - Gradient Boosting Regression')
plt.show()
```

✅ **Why?**
Identifies **which features most influence house prices**.

---

### 🔹 **Step 10: Hyperparameter Tuning (Optional)**

Using **GridSearchCV** for optimal parameters:

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5]
}

grid_search = GridSearchCV(GradientBoostingRegressor(random_state=42),
                           param_grid, cv=3, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best R-squared:", grid_search.best_score_)
```

---

### 🔹 **Step 11: Applicability of Gradient Boosting Regression**

| **When to use?**                   | **When not to use?**                         |
| ---------------------------------- | -------------------------------------------- |
| Highest predictive accuracy needed | Extremely small datasets                     |
| Complex non-linear relationships   | Real-time predictions with limited resources |
| Winning data science competitions  |                                              |

---

### 🔹 **Step 12: Advantages and Limitations**

✅ **Advantages:**

* High accuracy with proper tuning
* Handles **non-linear relationships** effectively
* Robust to overfitting with appropriate parameters

⚠️ **Limitations:**

* Slower training compared to Random Forest
* Requires **careful tuning** of hyperparameters

---

## ✅ **End of Full Case Study**

---

