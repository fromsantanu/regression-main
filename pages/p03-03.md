## ✅ **🔷 Section 3: Support Vector Regression (SVR)**

---

### 📝 **🔹 Step 0: Conceptual Overview**

* SVR tries to fit **best line within a margin**, effective for **high-dimensional and non-linear data** with kernel trick.

---

### 🔹 **Step 1: Import Libraries**

```python
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
```

---

### 🔹 **Step 2: Scale Data**

SVR requires **feature scaling**:

```python
sc_X = StandardScaler()
sc_y = StandardScaler()

X_scaled = sc_X.fit_transform(X)
y_scaled = sc_y.fit_transform(y.values.reshape(-1,1)).flatten()
```

---

### 🔹 **Step 3: Fit SVR Model**

```python
model = SVR(kernel='rbf')
model.fit(X_scaled, y_scaled)
```

---

### 🔹 **Step 4: Make Predictions**

```python
new_data_scaled = sc_X.transform(new_data)
predicted_scaled = model.predict(new_data_scaled)
predicted_price = sc_y.inverse_transform(predicted_scaled.reshape(-1,1))
print(predicted_price)
```

---

### 🔹 **Step 5: Applicability**

| **When to use?**                        | **When not to use?**                       |
| --------------------------------------- | ------------------------------------------ |
| High dimensional, non-linear regression | Large datasets (computationally intensive) |
| Smooth function approximation           | Interpretability needed                    |

---
### 📚 **🌟 Full Stepwise Case Study: Support Vector Regression (SVR)**

---

## ✅ **🔷 Case Study Topic**

**🔹 Predicting House Prices Using Support Vector Regression (SVR)**

---

### 📝 **🔹 Problem Statement**

You are given a dataset of **houses** with the following features:

* **Area:** Area in square feet
* **Bedrooms:** Number of bedrooms
* **Bathrooms:** Number of bathrooms
* **Stories:** Number of floors
* **Parking:** Number of parking spaces
* **Price:** House price (Target)

**Objective:**
Build an **SVR model** to predict **house prices** based on these features.

---

### 🔹 **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
```

---

### 🔹 **Step 2: Create Simulated Dataset**

We will use a similar dataset as in the Random Forest example for consistency:

```python
# Simulate data
np.random.seed(42)
data = {
    'Area': np.random.randint(500, 3500, 100),
    'Bedrooms': np.random.randint(1, 5, 100),
    'Bathrooms': np.random.randint(1, 4, 100),
    'Stories': np.random.randint(1, 3, 100),
    'Parking': np.random.randint(0, 3, 100)
}

df = pd.DataFrame(data)
# Create target variable with some random noise
df['Price'] = (df['Area'] * 300) + (df['Bedrooms'] * 50000) + (df['Bathrooms'] * 30000) + (df['Stories'] * 25000) + (df['Parking'] * 15000) + np.random.randint(-20000, 20000, 100)

print(df.head())
```

---

### 🔹 **Step 3: Define Features and Target**

```python
X = df.drop('Price', axis=1)
y = df['Price']
```

---

### 🔹 **Step 4: Train-Test Split**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

### 🔹 **Step 5: Feature Scaling**

✅ **Why?**
SVR is **sensitive to feature scales**, so we must standardize:

```python
sc_X = StandardScaler()
sc_y = StandardScaler()

X_train_scaled = sc_X.fit_transform(X_train)
X_test_scaled = sc_X.transform(X_test)

# Reshape y for scaling
y_train_scaled = sc_y.fit_transform(y_train.values.reshape(-1,1)).flatten()
y_test_scaled = sc_y.transform(y_test.values.reshape(-1,1)).flatten()
```

---

### 🔹 **Step 6: Fit SVR Model**

```python
model = SVR(kernel='rbf')
model.fit(X_train_scaled, y_train_scaled)
```

✅ **Hyperparameter Explanation:**

| **Parameter** | **Role**                                                  |
| ------------- | --------------------------------------------------------- |
| `kernel`      | Type of kernel function (‘linear’, ‘poly’, ‘rbf’)         |
| `C`           | Regularization parameter (higher C = less regularization) |
| `epsilon`     | Defines margin of tolerance where no penalty is given     |

---

### 🔹 **Step 7: Make Predictions**

```python
y_pred_scaled = model.predict(X_test_scaled)
# Inverse transform to original price scale
y_pred = sc_y.inverse_transform(y_pred_scaled.reshape(-1,1)).flatten()

print(y_pred[:5])
```

---

### 🔹 **Step 8: Evaluate Model**

```python
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

✅ **Interpretation:**

* **MSE:** Average squared difference between predicted and actual prices.
* **R²:** Percentage of variance explained by the model.

---

### 🔹 **Step 9: Applicability of SVR**

| **When to use?**                                 | **When not to use?**                     |
| ------------------------------------------------ | ---------------------------------------- |
| High-dimensional, non-linear regression problems | Very large datasets (computational cost) |
| Requires smooth function approximation           | Interpretability required                |
| Sensitive to feature scaling                     |                                          |

---

### 🔹 **Step 10: Advantages and Limitations**

✅ **Advantages:**

* Effective for **non-linear relationships** with kernel trick
* Robust against overfitting with proper tuning

⚠️ **Limitations:**

* Computationally expensive for large datasets
* Requires **careful parameter tuning** and feature scaling

---

### 🔹 **Step 11: Hyperparameter Tuning (Optional)**

Using **GridSearchCV** to optimize SVR parameters:

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.1, 1, 10],
    'epsilon': [0.01, 0.1, 1],
    'kernel': ['rbf', 'linear']
}

grid_search = GridSearchCV(SVR(), param_grid, cv=3, scoring='r2')
grid_search.fit(X_train_scaled, y_train_scaled)

print("Best Parameters:", grid_search.best_params_)
print("Best R-squared:", grid_search.best_score_)
```

---

## ✅ **End of Full Case Study**

