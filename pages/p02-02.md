# 📚 **🌟 Polynomial Regression**

### **Introduction to Polynomial Regression**

Polynomial regression is an extension of linear regression that models the relationship between the independent variable(s) and the dependent variable as an *nth degree polynomial*. It is particularly useful when the data shows a **non-linear relationship** that cannot be adequately captured by a straight line.

**Key aspects of polynomial regression include:**

* **Equation Form:**
  The general form of a polynomial regression equation is:

  $$
  y = β_0 + β_1x + β_2x^2 + β_3x^3 + ... + β_nx^n + ε
  $$

  where:

  * *y* is the dependent variable,
  * *x* is the independent variable,
  * *β₀, β₁, ..., βₙ* are the coefficients,
  * *n* is the degree of the polynomial,
  * *ε* is the error term representing residuals.

* **Interpretation:**
  Polynomial regression fits a curved line to the data, allowing it to model more complex relationships. For example, a quadratic regression (*n=2*) captures a parabolic relationship, while higher degrees can model more bends in the data.

* **Key considerations:**

  * **Overfitting risk:** Higher-degree polynomials can fit the training data very well but may generalize poorly to new data.
  * **Model complexity:** Increasing the polynomial degree adds complexity, making interpretation less intuitive compared to linear regression.
  * **Transforming features:** Polynomial regression is implemented by creating polynomial features (e.g. *x²*, *x³*) and then applying linear regression to these transformed features.

* **Assumptions:**
  Polynomial regression shares the assumptions of linear regression regarding residuals:

  * Independence of observations
  * Homoscedasticity (constant variance of residuals)
  * Normally distributed residuals

* **Applications:**
  Polynomial regression is used when:

  * The relationship between variables is **curvilinear** rather than strictly linear (e.g. modeling growth rates, acceleration, or curvature effects).
  * A linear model underfits the data, but a non-linear model improves accuracy.

In practice, polynomial regression provides a balance between **flexibility and interpretability**, making it a valuable tool for modeling non-linear relationships within a regression framework.


### 📝 **🔹 Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area (X)** where the relationship is **non-linear (curved)**.

---

### 🔹 **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
```

---

### 🔹 **Step 2: Create Sample Data**

```python
# Creating a sample dataset with non-linear relationship
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Price': [150000, 210000, 290000, 400000, 550000]
}
df = pd.DataFrame(data)
print(df)
```

✅ **Why this method?**

* **Applicability:** When **data shows curvature**, not a straight line.

---

### 🔹 **Step 3: Visualize Data**

```python
plt.scatter(df['Area'], df['Price'], color='blue')
plt.xlabel('Area (sq ft)')
plt.ylabel('Price ($)')
plt.title('Area vs Price (Possible Curve)')
plt.show()
```

---

### 🔹 **Step 4: Transform Data to Polynomial Features**

```python
X = df[['Area']]  # Independent variable as 2D array
y = df['Price']

# Transform to polynomial features (degree=2 for quadratic)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
```

✅ **What happened here?**

* Adds **additional columns** for Area² to capture the curve.

---

### 🔹 **Step 5: Fit Polynomial Regression Model**

```python
model = LinearRegression()
model.fit(X_poly, y)
```

---

### 🔹 **Step 6: Check Model Parameters**

```python
print(f"Intercept: {model.intercept_}")
print(f"Coefficients: {model.coef_}")
```

✅ **Interpretation:**

* **Coefficients:** For Area⁰ (intercept), Area¹ (linear), Area² (quadratic).

---

### 🔹 **Step 7: Make Predictions**

```python
# Predict prices for given areas
new_area = pd.DataFrame({'Area': [1100, 1600]})
new_area_poly = poly.transform(new_area)
predicted_price = model.predict(new_area_poly)
print(predicted_price)
```

---

### 🔹 **Step 8: Plot Polynomial Regression Curve**

```python
# Scatter plot
plt.scatter(df['Area'], df['Price'], color='blue')

# Regression curve
area_range = np.linspace(df['Area'].min(), df['Area'].max(), 100).reshape(-1,1)
area_range_poly = poly.transform(area_range)
price_pred = model.predict(area_range_poly)

plt.plot(area_range, price_pred, color='red')
plt.xlabel('Area (sq ft)')
plt.ylabel('Price ($)')
plt.title('Polynomial Regression: Area vs Price')
plt.show()
```

---

### 🔹 **Step 9: Evaluate Model**

```python
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X_poly)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

✅ **Interpretation:**

* **R² near 1:** Good fit to non-linear data
* **MSE small:** Predictions close to actual

---

### 🔹 **Step 10: Applicability of Polynomial Regression**

| **When to use?**                           | **When not to use?**                    |
| ------------------------------------------ | --------------------------------------- |
| Relationship is **curved/non-linear**      | Overfitting risk if **degree too high** |
| Data shows **quadratic or cubic patterns** | When relationship is truly linear       |
| Adding polynomial terms improves fit       | For extrapolation far beyond data       |

---

### 📝 **End of Part 3**

---

