# ğŸ“š **ğŸŒŸ Bayesian Regression**

---

### ğŸ“ **ğŸ”¹ Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area (X)** using **Bayesian inference**, which incorporates prior beliefs and provides a **distribution of predictions** instead of point estimates.

---

### ğŸ”¹ **Step 1: Conceptual Overview**

âœ… **What is Bayesian Regression?**

* Uses **Bayesian statistics** to estimate the posterior distribution of regression parameters, combining:

  * **Prior distribution (beliefs before data)**
  * **Likelihood (data evidence)**
* Outputs **credible intervals (probabilistic confidence)** for parameters and predictions.

---

### ğŸ”¹ **Step 2: Import Libraries**

Using **`pymc3`** for Bayesian modeling (requires installation if not already available):

```python
import pandas as pd
import numpy as np
import pymc3 as pm
import matplotlib.pyplot as plt
```

---

### ğŸ”¹ **Step 3: Create Sample Data**

```python
# Sample data
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

---

### ğŸ”¹ **Step 4: Define Bayesian Regression Model**

```python
X = df['Area'].values
y = df['Price'].values

# Standardizing X for better convergence
X_std = (X - X.mean()) / X.std()

with pm.Model() as model:
    # Priors for intercept and slope
    intercept = pm.Normal('Intercept', mu=0, sigma=1e6)
    slope = pm.Normal('Slope', mu=0, sigma=1e6)
    sigma = pm.HalfNormal('Sigma', sigma=1e6)
    
    # Expected value of outcome
    mu = intercept + slope * X_std
    
    # Likelihood
    Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Sampling from posterior
    trace = pm.sample(2000, tune=1000, cores=1, return_inferencedata=True)
```

---

### ğŸ”¹ **Step 5: Summarize Posterior Estimates**

```python
import arviz as az

az.summary(trace, hdi_prob=0.95)
```

âœ… **Interpretation:**

* Posterior mean, standard deviation, and **credible intervals (HDI)** for each parameter.

---

### ğŸ”¹ **Step 6: Plot Posterior Distributions**

```python
az.plot_posterior(trace)
plt.show()
```

---

### ğŸ”¹ **Step 7: Make Predictions**

```python
# Extract posterior samples
intercept_samples = trace.posterior['Intercept'].values.flatten()
slope_samples = trace.posterior['Slope'].values.flatten()

# Predict for a new standardized area (e.g., 1100 sq ft)
new_area = (1100 - X.mean()) / X.std()
predictions = intercept_samples + slope_samples * new_area

# Summarize predictions
print(f"Predicted Price (mean): {np.mean(predictions)}")
print(f"95% Credible Interval: {np.percentile(predictions, [2.5, 97.5])}")
```

---

### ğŸ”¹ **Step 8: Applicability of Bayesian Regression**

| **When to use?**                                    | **When not to use?**                               |
| --------------------------------------------------- | -------------------------------------------------- |
| Incorporating **prior beliefs** or expert knowledge | Prior information is unavailable or unnecessary    |
| Need **probabilistic interpretation** of results    | Simple point estimates are sufficient              |
| Small sample sizes with strong priors               | Large datasets where classical regression suffices |
| Complex hierarchical models                         |                                                    |

---

### ğŸ”¹ **Step 9: Advantages**

âœ… Provides:

* **Full posterior distribution** of parameters.
* **Credible intervals** (more intuitive than frequentist confidence intervals).
* Flexibility in **complex modeling** (hierarchical, latent variables).

---

### ğŸ“ **End of Part 12**

---

