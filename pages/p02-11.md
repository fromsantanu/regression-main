# üìö **üåü Bayesian Regression**

### **Introduction to Bayesian Regression**

Bayesian Regression is a regression analysis approach grounded in **Bayesian statistics**, where model parameters are treated as random variables with probability distributions rather than fixed but unknown quantities. It combines **prior knowledge** with observed data to produce a **posterior distribution** of the model parameters.

**Key aspects of Bayesian Regression include:**

* **Conceptual Framework:**
  Unlike traditional frequentist regression, Bayesian regression incorporates:

  * **Prior distribution (Prior):** Represents beliefs about parameter values before observing data.
  * **Likelihood:** The probability of observed data given the parameters.
  * **Posterior distribution (Posterior):** Updated beliefs about parameters after observing data, calculated using Bayes‚Äô theorem:

  $$
  \text{Posterior} \propto \text{Likelihood} \times \text{Prior}
  $$

* **Interpretation:**

  * Provides a **full probability distribution** for each parameter rather than a single point estimate, allowing direct probabilistic interpretations (e.g. ‚ÄúThere is a 95% probability that the parameter lies within this interval‚Äù).
  * Credible intervals replace frequentist confidence intervals, offering intuitive interpretations for decision-making.

* **Key advantages:**

  * **Incorporates prior knowledge:** Useful in scenarios with limited data or where expert knowledge is available.
  * **Uncertainty quantification:** Provides a natural way to quantify uncertainty in parameter estimates and predictions.
  * **Flexibility:** Can model complex hierarchical structures and adapt to a wide range of data scenarios.

* **Limitations:**

  * **Computational intensity:** Requires advanced algorithms like Markov Chain Monte Carlo (MCMC) for estimating posterior distributions, which can be computationally expensive.
  * **Choice of prior:** Results can be sensitive to the choice of prior, particularly with small datasets, making prior selection crucial.

* **Applications:**
  Bayesian Regression is widely used in:

  * **Medical and clinical research:** Incorporating prior studies or expert beliefs.
  * **Econometrics and finance:** Modeling uncertainty in forecasts and parameter estimates.
  * **Machine learning:** Serving as the foundation for Bayesian linear regression and probabilistic programming models.
  * **Environmental sciences and engineering:** When data are scarce or uncertain and prior information enhances model robustness.

Overall, Bayesian Regression offers a **powerful alternative to traditional regression**, enabling data analysts and researchers to integrate prior knowledge, quantify uncertainty more naturally, and model complex data structures within a probabilistic framework.

---


### üìù **üîπ Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area (X)** using **Bayesian inference**, which incorporates prior beliefs and provides a **distribution of predictions** instead of point estimates.

---

### üîπ **Step 1: Conceptual Overview**

‚úÖ **What is Bayesian Regression?**

* Uses **Bayesian statistics** to estimate the posterior distribution of regression parameters, combining:

  * **Prior distribution (beliefs before data)**
  * **Likelihood (data evidence)**
* Outputs **credible intervals (probabilistic confidence)** for parameters and predictions.

---

### üîπ **Step 2: Import Libraries**

Using **`pymc3`** for Bayesian modeling (requires installation if not already available):

```python
import pandas as pd
import numpy as np
import pymc3 as pm
import matplotlib.pyplot as plt
```

---

### üîπ **Step 3: Create Sample Data**

```python
# Sample data
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

---

### üîπ **Step 4: Define Bayesian Regression Model**

```python
X = df['Area'].values
y = df['Price'].values

# Standardizing X for better convergence
X_std = (X - X.mean()) / X.std()

with pm.Model() as model:
    # Priors for intercept and slope
    intercept = pm.Normal('Intercept', mu=0, sigma=1e6)
    slope = pm.Normal('Slope', mu=0, sigma=1e6)
    sigma = pm.HalfNormal('Sigma', sigma=1e6)
    
    # Expected value of outcome
    mu = intercept + slope * X_std
    
    # Likelihood
    Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma, observed=y)
    
    # Sampling from posterior
    trace = pm.sample(2000, tune=1000, cores=1, return_inferencedata=True)
```

---

### üîπ **Step 5: Summarize Posterior Estimates**

```python
import arviz as az

az.summary(trace, hdi_prob=0.95)
```

‚úÖ **Interpretation:**

* Posterior mean, standard deviation, and **credible intervals (HDI)** for each parameter.

---

### üîπ **Step 6: Plot Posterior Distributions**

```python
az.plot_posterior(trace)
plt.show()
```

---

### üîπ **Step 7: Make Predictions**

```python
# Extract posterior samples
intercept_samples = trace.posterior['Intercept'].values.flatten()
slope_samples = trace.posterior['Slope'].values.flatten()

# Predict for a new standardized area (e.g., 1100 sq ft)
new_area = (1100 - X.mean()) / X.std()
predictions = intercept_samples + slope_samples * new_area

# Summarize predictions
print(f"Predicted Price (mean): {np.mean(predictions)}")
print(f"95% Credible Interval: {np.percentile(predictions, [2.5, 97.5])}")
```

---

### üîπ **Step 8: Applicability of Bayesian Regression**

| **When to use?**                                    | **When not to use?**                               |
| --------------------------------------------------- | -------------------------------------------------- |
| Incorporating **prior beliefs** or expert knowledge | Prior information is unavailable or unnecessary    |
| Need **probabilistic interpretation** of results    | Simple point estimates are sufficient              |
| Small sample sizes with strong priors               | Large datasets where classical regression suffices |
| Complex hierarchical models                         |                                                    |

---

### üîπ **Step 9: Advantages**

‚úÖ Provides:

* **Full posterior distribution** of parameters.
* **Credible intervals** (more intuitive than frequentist confidence intervals).
* Flexibility in **complex modeling** (hierarchical, latent variables).

---

### üìù **End of Part 12**

---

