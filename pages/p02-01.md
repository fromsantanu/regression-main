# 📚 **🌟 Simple Linear Regression**

### **Introduction to Linear Regression Methods**

Linear regression is one of the most fundamental and widely used techniques in statistics and data analysis. It models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The core idea is to understand how the dependent variable changes when any one of the independent variables is varied, keeping others constant.

**Key aspects of linear regression include:**

* **Simplicity and Interpretability:** Linear regression provides a straightforward approach to quantify relationships, making it easily interpretable even for non-technical stakeholders.
* **Predictive Power:** Despite its simplicity, it serves as a strong baseline model for prediction tasks and as a building block for more complex models.
* **Types of Linear Regression:**

  * **Simple Linear Regression:** Involves one independent variable predicting a continuous dependent variable.
  * **Multiple Linear Regression:** Extends to scenarios with two or more independent variables.
  * **Regularized Linear Regression Methods:** Such as Ridge and Lasso Regression, which are used to prevent overfitting by adding penalty terms to the regression equation.

Linear regression assumptions include linearity, independence, homoscedasticity, and normality of residuals. Understanding these assumptions is critical to applying the model correctly and interpreting results effectively.

In practice, linear regression is used for applications ranging from **business forecasting** and **risk analysis** to **scientific research** and **engineering**, making it an essential technique for any data analyst or researcher.

---

### 📝 **🔹 Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area in sq. ft. (X)**.

---

### 🔹 **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
```

---

### 🔹 **Step 2: Create Sample Data**

```python
# Creating a simple dataset
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

✅ **Why this method?**

* **Applicability:** When **one independent variable** is used to predict a **continuous dependent variable** with **linear relationship**.

---

### 🔹 **Step 3: Visualize Data**

```python
plt.scatter(df['Area'], df['Price'], color='blue')
plt.xlabel('Area (sq ft)')
plt.ylabel('Price ($)')
plt.title('Area vs Price')
plt.show()
```

---

### 🔹 **Step 4: Prepare Data for Model**

```python
X = df[['Area']]  # Independent variable as 2D array
y = df['Price']   # Dependent variable as 1D array
```

---

### 🔹 **Step 5: Fit Simple Linear Regression Model**

```python
model = LinearRegression()
model.fit(X, y)
```

---

### 🔹 **Step 6: Check Model Parameters**

```python
print(f"Intercept (a): {model.intercept_}")
print(f"Slope (b): {model.coef_[0]}")
```

---

### 🔹 **Step 7: Make Predictions**

```python
# Predict price for new area values
new_area = pd.DataFrame({'Area': [1100, 1600]})
predicted_price = model.predict(new_area)
print(predicted_price)
```

---

### 🔹 **Step 8: Plot Regression Line**

```python
plt.scatter(df['Area'], df['Price'], color='blue')
plt.plot(df['Area'], model.predict(X), color='red')  # Regression line
plt.xlabel('Area (sq ft)')
plt.ylabel('Price ($)')
plt.title('Area vs Price with Regression Line')
plt.show()
```

---

### 🔹 **Step 9: Evaluate Model**

```python
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

✅ **Interpretation:**

* **R² near 1:** Strong fit
* **MSE small:** Predictions close to actual values

---

### 🔹 **Step 10: Applicability of Simple Linear Regression**

| **When to use?**                                             | **When not to use?**              |
| ------------------------------------------------------------ | --------------------------------- |
| Predicting **continuous outcomes** based on **one variable** | Relationship is **not linear**    |
| Relationship is approximately **straight line**              | There are **multiple predictors** |
| Checking **trend or association strength**                   | High variance in residuals        |

---

### 📝 **End of Part 1**

---

# 📚 **🌟 Multiple Linear Regression**

---

### 📝 **🔹 Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area (X1)** and **number of bedrooms (X2)**.

---

### 🔹 **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
```

---

### 🔹 **Step 2: Create Sample Data**

```python
# Creating a sample dataset
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

✅ **Why this method?**

* **Applicability:** When **more than one independent variable** is used to predict a **continuous dependent variable**.

---

### 🔹 **Step 3: Prepare Data for Model**

```python
X = df[['Area', 'Bedrooms']]  # Multiple independent variables
y = df['Price']               # Dependent variable
```

---

### 🔹 **Step 4: Fit Multiple Linear Regression Model**

```python
model = LinearRegression()
model.fit(X, y)
```

---

### 🔹 **Step 5: Check Model Parameters**

```python
print(f"Intercept: {model.intercept_}")
print(f"Coefficients: {model.coef_}")
```

✅ **Interpretation:**

* **Intercept:** Base price when all predictors are 0
* **Coefficients:** Effect of each predictor on price

---

### 🔹 **Step 6: Make Predictions**

```python
# Predict price for new area and bedrooms
new_data = pd.DataFrame({
    'Area': [1100, 1600],
    'Bedrooms': [2, 3]
})
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### 🔹 **Step 7: Evaluate Model**

```python
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

✅ **Interpretation:**

* **R² near 1:** Strong fit
* **MSE small:** Predictions close to actual values

---

### 🔹 **Step 8: Applicability of Multiple Linear Regression**

| **When to use?**                                                    | **When not to use?**                                     |
| ------------------------------------------------------------------- | -------------------------------------------------------- |
| Predicting **continuous outcomes** based on **multiple predictors** | Predictors are **highly correlated** (multicollinearity) |
| Relationships are approximately **linear**                          | Relationship is **not linear**                           |
| Checking **combined effects** of variables                          | Data has **non-linear trends**                           |

---

### 📝 **End of Part 2**

---
