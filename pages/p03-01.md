

## âœ… **ğŸ”· Section 1: Decision Tree Regression**

---

### ğŸ“ **ğŸ”¹ Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area and number of bedrooms** using **Decision Tree Regression**.

---

### ğŸ”¹ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
```

---

### ğŸ”¹ **Step 2: Create Sample Data**

```python
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

---

### ğŸ”¹ **Step 3: Prepare Data**

```python
X = df[['Area', 'Bedrooms']]
y = df['Price']
```

---

### ğŸ”¹ **Step 4: Fit Decision Tree Regressor**

```python
model = DecisionTreeRegressor(random_state=0)
model.fit(X, y)
```

---

### ğŸ”¹ **Step 5: Make Predictions**

```python
new_data = pd.DataFrame({
    'Area': [1100, 1600],
    'Bedrooms': [2, 3]
})
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### ğŸ”¹ **Step 6: Visualize Tree (Optional)**

```python
from sklearn.tree import plot_tree

plt.figure(figsize=(12,6))
plot_tree(model, feature_names=['Area', 'Bedrooms'], filled=True)
plt.show()
```

---

### ğŸ”¹ **Step 7: Evaluate Model**

```python
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

---

### ğŸ”¹ **Step 8: Applicability**

| **When to use?**                         | **When not to use?**                            |
| ---------------------------------------- | ----------------------------------------------- |
| Non-linear or complex relationships      | Small datasets prone to **overfitting**         |
| Interpretability (decision rules) needed | When **smooth continuous prediction** is needed |

---


## âœ… **ğŸ”· Section 3: Support Vector Regression (SVR)**

---

### ğŸ“ **ğŸ”¹ Step 0: Conceptual Overview**

* SVR tries to fit **best line within a margin**, effective for **high-dimensional and non-linear data** with kernel trick.

---

### ğŸ”¹ **Step 1: Import Libraries**

```python
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
```

---

### ğŸ”¹ **Step 2: Scale Data**

SVR requires **feature scaling**:

```python
sc_X = StandardScaler()
sc_y = StandardScaler()

X_scaled = sc_X.fit_transform(X)
y_scaled = sc_y.fit_transform(y.values.reshape(-1,1)).flatten()
```

---

### ğŸ”¹ **Step 3: Fit SVR Model**

```python
model = SVR(kernel='rbf')
model.fit(X_scaled, y_scaled)
```

---

### ğŸ”¹ **Step 4: Make Predictions**

```python
new_data_scaled = sc_X.transform(new_data)
predicted_scaled = model.predict(new_data_scaled)
predicted_price = sc_y.inverse_transform(predicted_scaled.reshape(-1,1))
print(predicted_price)
```

---

### ğŸ”¹ **Step 5: Applicability**

| **When to use?**                        | **When not to use?**                       |
| --------------------------------------- | ------------------------------------------ |
| High dimensional, non-linear regression | Large datasets (computationally intensive) |
| Smooth function approximation           | Interpretability needed                    |

---

## âœ… **ğŸ”· Section 4: K-Nearest Neighbors Regression**

---

### ğŸ“ **ğŸ”¹ Step 0: Import Library**

```python
from sklearn.neighbors import KNeighborsRegressor
```

---

### ğŸ”¹ **Step 1: Fit Model**

```python
model = KNeighborsRegressor(n_neighbors=3)
model.fit(X, y)
```

---

### ğŸ”¹ **Step 2: Make Predictions**

```python
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### ğŸ”¹ **Step 3: Applicability**

| **When to use?**                | **When not to use?**                            |
| ------------------------------- | ----------------------------------------------- |
| Simple, local prediction needed | High dimensional data (curse of dimensionality) |

---

## âœ… **ğŸ”· Section 5: Gradient Boosting Regression**

---

### ğŸ“ **ğŸ”¹ Step 0: Import Library**

```python
from sklearn.ensemble import GradientBoostingRegressor
```

---

### ğŸ”¹ **Step 1: Fit Model**

```python
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=0)
model.fit(X, y)
```

---

### ğŸ”¹ **Step 2: Make Predictions**

```python
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### ğŸ”¹ **Step 3: Applicability**

| **When to use?**                 | **When not to use?**     |
| -------------------------------- | ------------------------ |
| Highest accuracy needed          | Extremely small datasets |
| Non-linear complex relationships |                          |

---

## âœ… **ğŸ”· Section 6: Neural Network Regression**

---

### ğŸ“ **ğŸ”¹ Step 0: Import Libraries**

```python
from sklearn.neural_network import MLPRegressor
```

---

### ğŸ”¹ **Step 1: Fit Model**

```python
model = MLPRegressor(hidden_layer_sizes=(10,10), max_iter=1000, random_state=0)
model.fit(X, y)
```

---

### ğŸ”¹ **Step 2: Make Predictions**

```python
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### ğŸ”¹ **Step 3: Applicability**

| **When to use?**             | **When not to use?**    |
| ---------------------------- | ----------------------- |
| Complex, non-linear patterns | Small datasets          |
| Deep learning tasks          | Interpretability needed |

---

## ğŸ“ **ğŸ”· Final Summary**

| **Method**        | **Key Advantage**                      | **Limitation**                       |
| ----------------- | -------------------------------------- | ------------------------------------ |
| Decision Tree     | Easy interpretation                    | Overfitting                          |
| Random Forest     | High accuracy, robust                  | Less interpretable                   |
| SVR               | High dimensional non-linear regression | Computational cost                   |
| KNN               | Simple, local                          | Poor in high dimensions              |
| Gradient Boosting | High predictive power                  | Slower training                      |
| Neural Networks   | Captures complex patterns              | Needs large data, less interpretable |

---

### âœ… **End of Part 13**

---

ğŸ’¡ Let me know if you wish for **full datasets, detailed case studies, hyperparameter tuning tutorials**, or **project ideas** using these advanced regression methods for your upcoming practice and classes.

