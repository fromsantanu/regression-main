

## âœ… **ðŸ”· Section 1: Decision Tree Regression**

---

### ðŸ“ **ðŸ”¹ Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area and number of bedrooms** using **Decision Tree Regression**.

---

### ðŸ”¹ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
```

---

### ðŸ”¹ **Step 2: Create Sample Data**

```python
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

---

### ðŸ”¹ **Step 3: Prepare Data**

```python
X = df[['Area', 'Bedrooms']]
y = df['Price']
```

---

### ðŸ”¹ **Step 4: Fit Decision Tree Regressor**

```python
model = DecisionTreeRegressor(random_state=0)
model.fit(X, y)
```

---

### ðŸ”¹ **Step 5: Make Predictions**

```python
new_data = pd.DataFrame({
    'Area': [1100, 1600],
    'Bedrooms': [2, 3]
})
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### ðŸ”¹ **Step 6: Visualize Tree (Optional)**

```python
from sklearn.tree import plot_tree

plt.figure(figsize=(12,6))
plot_tree(model, feature_names=['Area', 'Bedrooms'], filled=True)
plt.show()
```

---

### ðŸ”¹ **Step 7: Evaluate Model**

```python
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

---

### ðŸ”¹ **Step 8: Applicability**

| **When to use?**                         | **When not to use?**                            |
| ---------------------------------------- | ----------------------------------------------- |
| Non-linear or complex relationships      | Small datasets prone to **overfitting**         |
| Interpretability (decision rules) needed | When **smooth continuous prediction** is needed |

---

### ðŸ“š **ðŸŒŸ Full Stepwise Case Study: Decision Tree Regression**

---

## âœ… **ðŸ”· Case Study Topic**

**ðŸ”¹ Predicting Car Prices Using Decision Tree Regression**

---

### ðŸ“ **ðŸ”¹ Problem Statement**

You are given a dataset of **used cars** with the following features:

* **Year:** Year of manufacture
* **Present\_Price:** Current ex-showroom price
* **Kms\_Driven:** Kilometers driven
* **Owner:** Number of previous owners (0,1,2,3)
* **Fuel\_Type:** Petrol/Diesel/CNG
* **Seller\_Type:** Dealer/Individual
* **Transmission:** Manual/Automatic
* **Selling\_Price:** Price at which car is being sold (Target)

**Objective:**
Build a **Decision Tree Regression model** to predict **Selling Price** based on other features.

---

### ðŸ”¹ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
```

---

### ðŸ”¹ **Step 2: Load Dataset**

Using a simulated dataset similar to the **CarDekho dataset**:

```python
# Load from URL or local CSV
url = 'https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv'  # Replace with your dataset if available
df = pd.read_csv(url)
df.head()
```

ðŸ”¹ **Note:** If this dataset structure differs, for your classes, use the cleaned **CarDekho** dataset available from Kaggle, or I can prepare a simulated data for you if needed.

---

### ðŸ”¹ **Step 3: Data Cleaning & Preprocessing**

```python
# Check data types and missing values
print(df.info())
print(df.isnull().sum())

# Drop rows with missing target or critical values
df.dropna(subset=['Price'], inplace=True)

# For this example, select subset features or rename to match case study
df.rename(columns={'Price':'Selling_Price','Horsepower':'Present_Price','MPG.city':'Kms_Driven'}, inplace=True)

# Convert categorical variables
df['Fuel_Type'] = np.where(df['Fuel.tank.capacity']>15, 'Petrol', 'Diesel')
df['Seller_Type'] = np.where(df['DriveTrain']=='Front', 'Dealer', 'Individual')
df['Transmission'] = np.where(df['Man.trans.avail']=='Yes', 'Manual', 'Automatic')

# Drop unused columns
df = df[['Selling_Price','Present_Price','Kms_Driven','Fuel_Type','Seller_Type','Transmission']]
df.dropna(inplace=True)
```

---

### ðŸ”¹ **Step 4: Encoding Categorical Variables**

```python
df = pd.get_dummies(df, drop_first=True)
print(df.head())
```

âœ… **Why?**
Decision Tree in scikit-learn requires **numeric input**; one-hot encoding converts categories to binary variables.

---

### ðŸ”¹ **Step 5: Define Features and Target**

```python
X = df.drop('Selling_Price', axis=1)
y = df['Selling_Price']
```

---

### ðŸ”¹ **Step 6: Train-Test Split**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

### ðŸ”¹ **Step 7: Fit Decision Tree Regressor**

```python
model = DecisionTreeRegressor(random_state=42, max_depth=4)  # Limiting depth to avoid overfitting
model.fit(X_train, y_train)
```

âœ… **Hyperparameter Explanation:**

| **Parameter**       | **Role**                                 |
| ------------------- | ---------------------------------------- |
| `max_depth`         | Limits tree depth to control overfitting |
| `min_samples_split` | Minimum samples to split a node          |
| `min_samples_leaf`  | Minimum samples in leaf node             |

---

### ðŸ”¹ **Step 8: Make Predictions**

```python
y_pred = model.predict(X_test)
print(y_pred[:5])
```

---

### ðŸ”¹ **Step 9: Evaluate Model**

```python
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

âœ… **Interpretation:**

* **MSE:** Average squared difference between predicted and actual prices.
* **RÂ²:** Percentage of variance explained by the model.

---

### ðŸ”¹ **Step 10: Visualize Decision Tree**

```python
plt.figure(figsize=(20,10))
plot_tree(model, feature_names=X.columns, filled=True)
plt.show()
```

âœ… **Why visualize?**
Shows **decision rules** used for prediction â€“ a strong advantage of decision trees for interpretability in teaching.

---

### ðŸ”¹ **Step 11: Feature Importance**

```python
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=False).plot(kind='bar')
plt.title('Feature Importance')
plt.show()
```

âœ… **Why?**
Helps identify **which features impact selling price most**.

---

### ðŸ”¹ **Step 12: Applicability of Decision Tree Regression**

| **When to use?**                     | **When not to use?**                         |
| ------------------------------------ | -------------------------------------------- |
| Clear decision rules needed          | Data is very small and risk of overfitting   |
| Non-linear relationships             | Prediction needs to be smooth and continuous |
| Easy interpretation for stakeholders |                                              |

---

## âœ… **End of Full Case Study**









