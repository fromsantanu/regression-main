### ðŸ“š **ðŸŒŸ Theoretical Explanation: How Decision Tree Regression Works**

---

### âœ… **1. What is Decision Tree Regression?**

A **Decision Tree Regression** is a **non-parametric supervised learning method** used to predict **continuous target variables** by learning decision rules from input features.

âœ… **Key idea:**
It **splits the data into smaller and smaller subsets** based on feature values and fits a simple prediction (usually the mean) within each final subset (leaf).

---

### ðŸ”¹ **2. How Does It Work?**

#### **Step-by-Step Process:**

âœ… **Step 1: Start with the entire dataset**

* The **root node** represents the entire data.
* The algorithm looks for **all possible splits** across all features.

---

âœ… **Step 2: Choose the best split**

* For each possible split:

  * Divide data into two parts based on the condition (e.g., Area < 1200 sq ft).
  * Calculate the **variance reduction (or reduction in mean squared error)**.

**Mathematically:**

$$
Reduction = Variance_{parent} - \left( \frac{n_{left}}{n_{total}} Variance_{left} + \frac{n_{right}}{n_{total}} Variance_{right} \right)
$$

* The split with **maximum variance reduction** is selected.

---

âœ… **Step 3: Create branches**

* Data is divided into two child nodes.
* The splitting process is repeated **recursively** for each child node.

---

âœ… **Step 4: Stopping criteria**

The tree continues to grow until one or more stopping conditions are met:

* **Maximum depth** reached (prevents overfitting).
* **Minimum number of samples** in a node (e.g. min\_samples\_split).
* **Minimum reduction in error** achieved by a split.

---

âœ… **Step 5: Assign prediction values**

* For **regression**, each **leaf node predicts the mean** of target values in that subset.

---

### ðŸ”¹ **3. Example (Simple)**

**Dataset:**

| Area (sq ft) | Bedrooms | Price (\$) |
| ------------ | -------- | ---------- |
| 800          | 2        | 150,000    |
| 1200         | 3        | 200,000    |
| 1500         | 3        | 250,000    |
| 2000         | 4        | 350,000    |

âœ… **First Split:**

* It may split at **Area < 1350**:

  * Left Node: \[800, 1200] â†’ Mean price = \$175,000
  * Right Node: \[1500, 2000] â†’ Mean price = \$300,000

Further splits continue within each branch if stopping criteria allow.

---

### ðŸ”¹ **4. Advantages**

âœ… **Key strengths:**

* **Easy to interpret** (visual decision rules)
* Handles **non-linear relationships**
* No need for feature scaling or normalization
* Can handle **both numerical and categorical features**

---

### ðŸ”¹ **5. Limitations**

âš ï¸ **Key weaknesses:**

* **Prone to overfitting** if not pruned (deep trees fit training data too closely)
* **Unstable:** small changes in data can lead to different splits
* Less accurate compared to ensemble methods (Random Forest, Gradient Boosting)

---

### ðŸ”¹ **6. Summary of Working Principle**

1. **Select feature and split point** that best reduces variance.
2. **Split data** into child nodes based on the condition.
3. **Repeat recursively** for each node (top-down approach).
4. **Stop splitting** based on pre-set conditions.
5. **Predict output** as **mean of target values** within each leaf node.

---

## âœ… **ðŸ”· Section 1: Decision Tree Regression**

---

### ðŸ“ **ðŸ”¹ Step 0: Problem Setup**

**Example Problem:**
Predict **house price (Y)** based on **area and number of bedrooms** using **Decision Tree Regression**.

---

### ðŸ”¹ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
```

---

### ðŸ”¹ **Step 2: Create Sample Data**

```python
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

---

### ðŸ”¹ **Step 3: Prepare Data**

```python
X = df[['Area', 'Bedrooms']]
y = df['Price']
```

---

### ðŸ”¹ **Step 4: Fit Decision Tree Regressor**

```python
model = DecisionTreeRegressor(random_state=0)
model.fit(X, y)
```

---

### ðŸ”¹ **Step 5: Make Predictions**

```python
new_data = pd.DataFrame({
    'Area': [1100, 1600],
    'Bedrooms': [2, 3]
})
predicted_price = model.predict(new_data)
print(predicted_price)
```

---

### ðŸ”¹ **Step 6: Visualize Tree (Optional)**

```python
from sklearn.tree import plot_tree

plt.figure(figsize=(12,6))
plot_tree(model, feature_names=['Area', 'Bedrooms'], filled=True)
plt.show()
```

---

### ðŸ”¹ **Step 7: Evaluate Model**

```python
from sklearn.metrics import mean_squared_error, r2_score

y_pred = model.predict(X)
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

---

### ðŸ”¹ **Step 8: Applicability**

| **When to use?**                         | **When not to use?**                            |
| ---------------------------------------- | ----------------------------------------------- |
| Non-linear or complex relationships      | Small datasets prone to **overfitting**         |
| Interpretability (decision rules) needed | When **smooth continuous prediction** is needed |

---

### ðŸ“š **ðŸŒŸ Full Stepwise Case Study: Decision Tree Regression**

---

## âœ… **ðŸ”· Case Study Topic**

**ðŸ”¹ Predicting Car Prices Using Decision Tree Regression**

---

### ðŸ“ **ðŸ”¹ Problem Statement**

You are given a dataset of **used cars** with the following features:

* **Year:** Year of manufacture
* **Present\_Price:** Current ex-showroom price
* **Kms\_Driven:** Kilometers driven
* **Owner:** Number of previous owners (0,1,2,3)
* **Fuel\_Type:** Petrol/Diesel/CNG
* **Seller\_Type:** Dealer/Individual
* **Transmission:** Manual/Automatic
* **Selling\_Price:** Price at which car is being sold (Target)

**Objective:**
Build a **Decision Tree Regression model** to predict **Selling Price** based on other features.

---

### ðŸ”¹ **Step 1: Import Libraries**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
```

---

### ðŸ”¹ **Step 2: Load Dataset**

Using a simulated dataset similar to the **CarDekho dataset**:

```python
# Load from URL or local CSV
url = 'https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv'  # Replace with your dataset if available
df = pd.read_csv(url)
df.head()
```

ðŸ”¹ **Note:** If this dataset structure differs, for your classes, use the cleaned **CarDekho** dataset available from Kaggle, or I can prepare a simulated data for you if needed.

---

### ðŸ”¹ **Step 3: Data Cleaning & Preprocessing**

```python
# Check data types and missing values
print(df.info())
print(df.isnull().sum())

# Drop rows with missing target or critical values
df.dropna(subset=['Price'], inplace=True)

# For this example, select subset features or rename to match case study
df.rename(columns={'Price':'Selling_Price','Horsepower':'Present_Price','MPG.city':'Kms_Driven'}, inplace=True)

# Convert categorical variables
df['Fuel_Type'] = np.where(df['Fuel.tank.capacity']>15, 'Petrol', 'Diesel')
df['Seller_Type'] = np.where(df['DriveTrain']=='Front', 'Dealer', 'Individual')
df['Transmission'] = np.where(df['Man.trans.avail']=='Yes', 'Manual', 'Automatic')

# Drop unused columns
df = df[['Selling_Price','Present_Price','Kms_Driven','Fuel_Type','Seller_Type','Transmission']]
df.dropna(inplace=True)
```

---

### ðŸ”¹ **Step 4: Encoding Categorical Variables**

```python
df = pd.get_dummies(df, drop_first=True)
print(df.head())
```

âœ… **Why?**
Decision Tree in scikit-learn requires **numeric input**; one-hot encoding converts categories to binary variables.

---

### ðŸ”¹ **Step 5: Define Features and Target**

```python
X = df.drop('Selling_Price', axis=1)
y = df['Selling_Price']
```

---

### ðŸ”¹ **Step 6: Train-Test Split**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

### ðŸ”¹ **Step 7: Fit Decision Tree Regressor**

```python
model = DecisionTreeRegressor(random_state=42, max_depth=4)  # Limiting depth to avoid overfitting
model.fit(X_train, y_train)
```

âœ… **Hyperparameter Explanation:**

| **Parameter**       | **Role**                                 |
| ------------------- | ---------------------------------------- |
| `max_depth`         | Limits tree depth to control overfitting |
| `min_samples_split` | Minimum samples to split a node          |
| `min_samples_leaf`  | Minimum samples in leaf node             |

---

### ðŸ”¹ **Step 8: Make Predictions**

```python
y_pred = model.predict(X_test)
print(y_pred[:5])
```

---

### ðŸ”¹ **Step 9: Evaluate Model**

```python
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")
```

âœ… **Interpretation:**

* **MSE:** Average squared difference between predicted and actual prices.
* **RÂ²:** Percentage of variance explained by the model.

---

### ðŸ”¹ **Step 10: Visualize Decision Tree**

```python
plt.figure(figsize=(20,10))
plot_tree(model, feature_names=X.columns, filled=True)
plt.show()
```

âœ… **Why visualize?**
Shows **decision rules** used for prediction â€“ a strong advantage of decision trees for interpretability in teaching.

---

### ðŸ”¹ **Step 11: Feature Importance**

```python
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=False).plot(kind='bar')
plt.title('Feature Importance')
plt.show()
```

âœ… **Why?**
Helps identify **which features impact selling price most**.

---

### ðŸ”¹ **Step 12: Applicability of Decision Tree Regression**

| **When to use?**                     | **When not to use?**                         |
| ------------------------------------ | -------------------------------------------- |
| Clear decision rules needed          | Data is very small and risk of overfitting   |
| Non-linear relationships             | Prediction needs to be smooth and continuous |
| Easy interpretation for stakeholders |                                              |

---

## âœ… **End of Full Case Study**









