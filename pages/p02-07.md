# üìö **üåü Stepwise Regression**

### **Introduction to Stepwise Regression**

Stepwise regression is a **model selection technique** used to identify a subset of predictor variables that best explains the variability in the dependent variable. It combines the process of **regression modeling with automatic variable selection**, adding or removing predictors based on specific statistical criteria.

**Key aspects of Stepwise Regression include:**

* **Types of Stepwise Methods:**
  There are three main approaches:

  * **Forward Selection:** Starts with no variables in the model and adds predictors one by one based on improvement in model fit (e.g. using p-values or AIC).
  * **Backward Elimination:** Starts with all candidate variables and removes the least significant predictors one by one.
  * **Stepwise Selection (Bidirectional Elimination):** A combination of forward and backward methods, adding or removing variables at each step based on significance.

* **Criteria for Selection:**
  Predictors are added or removed based on:

  * **p-values** from statistical tests (commonly used in basic stepwise methods), or
  * **Information criteria** such as AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion), which balance model fit and complexity.

* **Interpretation:**
  Stepwise regression helps to:

  * Identify the most relevant variables influencing the outcome.
  * Create simpler, more interpretable models by excluding redundant or insignificant predictors.

* **Key advantages:**

  * **Efficiency:** Automates variable selection, saving time in exploratory modeling.
  * **Model simplicity:** Results in more parsimonious models, reducing overfitting risks.

* **Limitations:**

  * **Risk of overfitting:** Especially in small datasets with many predictors.
  * **Instability:** Results can vary depending on data or order of variable entry.
  * **Ignores multicollinearity effects:** May include correlated variables without assessing their joint effect adequately.

* **Applications:**
  Stepwise regression is commonly used in:

  * **Exploratory modeling:** To identify important predictors before final model building.
  * **Applied research:** Where interpretability and simplicity are essential alongside predictive performance.
  * **Model refinement:** As an initial method to reduce dimensionality before applying advanced techniques.

Overall, stepwise regression is a **practical and widely used technique for feature selection and model simplification**, though it should be applied with caution, complemented by domain knowledge and validation techniques to ensure robust results.

---

### üìù **üîπ Step 0: Problem Setup**

**Example Problem:**
Select the **best combination of predictors** to predict **house price (Y)** from multiple features using **automated variable selection**.

---

### üîπ **Step 1: Conceptual Overview**

‚úÖ **What is Stepwise Regression?**

A **variable selection method** that adds or removes predictors based on their statistical significance, aiming for the **best model with minimum predictors**.

* **Forward Selection:** Starts with no variables, adds them one by one based on criteria.
* **Backward Elimination:** Starts with all variables, removes them one by one.
* **Bidirectional Elimination:** Combines both.

---

### üîπ **Step 2: Required Libraries**

`sklearn` does not have direct stepwise regression, but we can implement it using **statsmodels** or manual looping. Below is an implementation using **statsmodels and a forward selection approach**.

```python
import pandas as pd
import statsmodels.api as sm
```

---

### üîπ **Step 3: Create Sample Data**

```python
# Sample dataset
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Age': [5, 7, 10, 12, 15],
    'Distance': [2, 3, 5, 7, 8],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

---

### üîπ **Step 4: Forward Selection Function**

```python
def forward_selection(data, target, significance_level=0.05):
    initial_features = []
    remaining_features = list(data.columns)
    remaining_features.remove(target)
    selected_features = []
    
    while remaining_features:
        p_values = pd.Series(index=remaining_features, dtype=float)
        for feature in remaining_features:
            model = sm.OLS(data[target], sm.add_constant(data[initial_features + [feature]])).fit()
            p_values[feature] = model.pvalues[feature]
        
        min_p_value = p_values.min()
        if min_p_value < significance_level:
            best_feature = p_values.idxmin()
            initial_features.append(best_feature)
            remaining_features.remove(best_feature)
            selected_features.append(best_feature)
        else:
            break
    return selected_features
```

---

### üîπ **Step 5: Run Forward Selection**

```python
selected = forward_selection(df, 'Price')
print("Selected Features:", selected)
```

‚úÖ **Interpretation:**

* Returns the **list of variables that significantly predict** house price.

---

### üîπ **Step 6: Build Final Model**

```python
X = sm.add_constant(df[selected])
y = df['Price']

final_model = sm.OLS(y, X).fit()
print(final_model.summary())
```

---

### üîπ **Step 7: Applicability of Stepwise Regression**

| **When to use?**                                           | **When not to use?**                          |
| ---------------------------------------------------------- | --------------------------------------------- |
| Large number of predictors needing **automatic selection** | Small dataset with few predictors             |
| Feature selection in **exploratory modelling**             | When **theory-based selection** is preferable |
| Simplifying models without manual elimination              | Potential **overfitting or biased selection** |

---

### üîπ **Step 8: Limitations**

‚ö†Ô∏è **Stepwise methods:**

* Do not guarantee the **best possible model**.
* Can **overfit** in small samples.
* Ignore multicollinearity impact.

---

### üìù **End of Part 8**

---


