# ğŸ“š **ğŸŒŸ Stepwise Regression**

---

### ğŸ“ **ğŸ”¹ Step 0: Problem Setup**

**Example Problem:**
Select the **best combination of predictors** to predict **house price (Y)** from multiple features using **automated variable selection**.

---

### ğŸ”¹ **Step 1: Conceptual Overview**

âœ… **What is Stepwise Regression?**

A **variable selection method** that adds or removes predictors based on their statistical significance, aiming for the **best model with minimum predictors**.

* **Forward Selection:** Starts with no variables, adds them one by one based on criteria.
* **Backward Elimination:** Starts with all variables, removes them one by one.
* **Bidirectional Elimination:** Combines both.

---

### ğŸ”¹ **Step 2: Required Libraries**

`sklearn` does not have direct stepwise regression, but we can implement it using **statsmodels** or manual looping. Below is an implementation using **statsmodels and a forward selection approach**.

```python
import pandas as pd
import statsmodels.api as sm
```

---

### ğŸ”¹ **Step 3: Create Sample Data**

```python
# Sample dataset
data = {
    'Area': [500, 750, 1000, 1250, 1500],
    'Bedrooms': [1, 2, 2, 3, 3],
    'Age': [5, 7, 10, 12, 15],
    'Distance': [2, 3, 5, 7, 8],
    'Price': [150000, 200000, 250000, 300000, 350000]
}
df = pd.DataFrame(data)
print(df)
```

---

### ğŸ”¹ **Step 4: Forward Selection Function**

```python
def forward_selection(data, target, significance_level=0.05):
    initial_features = []
    remaining_features = list(data.columns)
    remaining_features.remove(target)
    selected_features = []
    
    while remaining_features:
        p_values = pd.Series(index=remaining_features, dtype=float)
        for feature in remaining_features:
            model = sm.OLS(data[target], sm.add_constant(data[initial_features + [feature]])).fit()
            p_values[feature] = model.pvalues[feature]
        
        min_p_value = p_values.min()
        if min_p_value < significance_level:
            best_feature = p_values.idxmin()
            initial_features.append(best_feature)
            remaining_features.remove(best_feature)
            selected_features.append(best_feature)
        else:
            break
    return selected_features
```

---

### ğŸ”¹ **Step 5: Run Forward Selection**

```python
selected = forward_selection(df, 'Price')
print("Selected Features:", selected)
```

âœ… **Interpretation:**

* Returns the **list of variables that significantly predict** house price.

---

### ğŸ”¹ **Step 6: Build Final Model**

```python
X = sm.add_constant(df[selected])
y = df['Price']

final_model = sm.OLS(y, X).fit()
print(final_model.summary())
```

---

### ğŸ”¹ **Step 7: Applicability of Stepwise Regression**

| **When to use?**                                           | **When not to use?**                          |
| ---------------------------------------------------------- | --------------------------------------------- |
| Large number of predictors needing **automatic selection** | Small dataset with few predictors             |
| Feature selection in **exploratory modelling**             | When **theory-based selection** is preferable |
| Simplifying models without manual elimination              | Potential **overfitting or biased selection** |

---

### ğŸ”¹ **Step 8: Limitations**

âš ï¸ **Stepwise methods:**

* Do not guarantee the **best possible model**.
* Can **overfit** in small samples.
* Ignore multicollinearity impact.

---

### ğŸ“ **End of Part 8**

---


